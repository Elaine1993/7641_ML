## Name: Yiying Zhang  GTID: yzhang3006.
## Assignmenet 1: Supervised Learning

import numpy as np
import pandas as pd
from sklearn.model_selection import cross_val_score, GridSearchCV, cross_validate, train_test_split
from sklearn import tree
from sklearn.metrics import accuracy_score, classification_report
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.preprocessing import StandardScaler, normalize
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import confusion_matrix
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt


######################################### Reading and Cleaning the Data ###############################################
# data resource1: https://archive.ics.uci.edu/ml/datasets/Online+Shoppers+Purchasing+Intention+Dataset
data1 = pd.read_csv('online_shoppers_intention.csv')

#Data cleaning: categorical data must be encoded in ML method like: Linear regression, SVM, Neural network.
data1[['Revenue', 'Weekend']] = (data1[['Revenue', 'Weekend']] == True).astype(int)
VisitorType_map = {'VisitorType': {"Returning_Visitor": 1, "Other": 2, "New_Visitor": 0}}
data1.replace(VisitorType_map, inplace= True)
Month_map =  {'Jan':1, 'Feb':2, 'Mar':3, 'Apr':4, 'May': 5,'June': 6, 'Jul':7, 'Aug':8, 'Sep':9, 'Oct':10, 'Nov': 11, 'Dec': 12}
data1.replace(Month_map, inplace= True)

# Separate out the x_data and y_data.
x_data1 = data1.loc[:, data1.columns != "Revenue"]
y_data1 = data1.loc[:, "Revenue"]
print("after-cleaning data", data1.head())

# data resource2: https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients
data2 = pd.read_excel("default_of_credit_card_clients.xls", skiprows=[1], index_col= 0)
print("before-cleaning data2\n", data2.head())

# Separate out the x_data and y_data.
x_data2 = data2.loc[:, data2.columns != "Y"]
y_data2 = data2.loc[:, "Y"]
#print("before-cleaning data2", data2.head())



# ############################################### Helper Functions ###################################################
# The random state to use while splitting the data.
random_state = 100

#######Split i% of the data into training and 1-i% into test sets. #######
# Use the train_test_split method in sklearn with the parameter 'shuffle' set to true and the 'random_state' set to 100.
def fixedsplitData(xdata, ydata):
     xTrain, xTest, yTrain, yTest = train_test_split(xdata, ydata, train_size = 0.7, random_state=random_state)
     return xTrain, xTest, yTrain, yTest

######### experiment_size: iteration for n time, each time, use change train size ########
def experiment_size(n, f):
    TrainResult1 = []
    TestResult1 = []
    TrainResult2 = []
    TestResult2 = []
    for i in range(1, n):
        a = i / n
        print("Train size (a) = ", a)
        trainScore1, testScore1 = f(x_data1, y_data1, a)
        TrainResult1.append(trainScore1)
        TestResult1.append(testScore1)
        trainScore2, testScore2 = f(x_data2, y_data2, a)
        TrainResult2.append(trainScore2)
        TestResult2.append(testScore2)
    print("TrainResult1", TrainResult1)
    print("TestResult1", TestResult1)
    print("TrainResult2", TrainResult2)
    print("TestResult2", TestResult2)
    print("\n")
    return TrainResult1, TestResult1, TrainResult2, TestResult2

######### experiment_parameter: iteration for n time, each time, change parameter########
def experiment_parameter(n, f):
    xTrain1, xTest1, yTrain1, yTest1= fixedsplitData(x_data1, y_data1)
    xTrain2, xTest2, yTrain2, yTest2= fixedsplitData(x_data2, y_data2)
    TrainResult_parameter1 = []
    TestResult_parameter1 = []
    TrainResult_parameter2 = []
    TestResult_parameter2 = []
    for i in range(1, n):
        print("parameter = ", i)
        trainScore1, testScore1 = f(xTrain1, xTest1, yTrain1, yTest1, i)
        TrainResult_parameter1.append(trainScore1)
        TestResult_parameter1.append(testScore1)
        trainScore2, testScore2 = f(xTrain2, xTest2, yTrain2, yTest2, i)
        TrainResult_parameter2.append(trainScore2)
        TestResult_parameter2.append(testScore2)
    print("TrainResult_parameter1 =", TrainResult_parameter1)
    print("TestResult_parameter1 =", TestResult_parameter1)
    print("TrainResult_parameter2 =", TrainResult_parameter2)
    print("TestResult_parameter2=", TestResult_parameter2)
    print("\n")
    return TrainResult_parameter1, TestResult_parameter1, TrainResult_parameter2, TestResult_parameter2


######### Plot: train size vs accuracy #############
# call after iteration is done and get data list
def plotAccuray_TrainSize(TrainResult1,TestResult1, TrainResult2,TestResult2,plotname, parameter_name):
    xrange = np.arange(0.01,1.0, 0.01)
    plt.plot(xrange, TrainResult1, label="TrainAccuracy-Dataset1")
    plt.plot(xrange, TestResult1, label="TestAccuracy-Dataset1")
    plt.plot(xrange, TrainResult2, label="TrainAccuracy-Dataset2")
    plt.plot(xrange, TestResult2, label="TestAccuracy-Dataset2")
    plt.legend()
    plt.xlabel(parameter_name)
    plt.ylabel("Accuracy")
    plt.title("Accuracy Score of " + str(plotname))
    plt.savefig("Accuracy Score of " + str(plotname) + ".png")
    plt.show()

def plotAccuray_parameter(TrainResult1,TestResult1, TrainResult2,TestResult2,plotname, parameter_name, n):
    xrange = np.arange(1,n)
    plt.plot(xrange, TrainResult1, label="TrainAccuracy-Dataset1")
    plt.plot(xrange, TestResult1, label="TestAccuracy-Dataset1")
    plt.plot(xrange, TrainResult2, label="TrainAccuracy-Dataset2")
    plt.plot(xrange, TestResult2, label="TestAccuracy-Dataset2")
    plt.legend()
    plt.xlabel(parameter_name)
    plt.ylabel("Accuracy")
    plt.title("Accuracy Score of " + str(plotname))
    plt.savefig("Accuracy Score of " + str(plotname) + ".png")
    plt.show()


################ cross validation ##############
def crossValidation(clasifier):
    return

# ############################################### Decision Tree ###################################################
def DTLearner(xdata, ydata, trainsize):
    xTrain, xTest, yTrain, yTest = train_test_split(xdata, ydata, train_size=trainsize, random_state=random_state)
    # Create a DT classifier and train it.
    dt = tree.DecisionTreeClassifier().fit(xTrain, yTrain)
    # Note: Round the output values greater than or equal to 0.5 to 1 and those less than 0.5 to 0. You can use y_predict.round() or any other method.
    trainScore = accuracy_score(yTrain, dt.predict(xTrain).round())
    testScore = accuracy_score(yTest, dt.predict(xTest).round())
    print('trainScore', trainScore)
    print('testScoret', testScore)
    return trainScore, testScore
def DTLearner_parameter(xTrain, xTest, yTrain, yTest, p):
    # Create a DT classifier and train it.
    dt = tree.DecisionTreeClassifier(max_depth = p).fit(xTrain, yTrain)
    trainScore = accuracy_score(yTrain, dt.predict(xTrain).round())
    testScore = accuracy_score(yTest, dt.predict(xTest).round())
    print('trainScore', trainScore)
    print('testScoret', testScore)
    return trainScore, testScore

# ############################################### Decision Tree Experiment with 2 data_sets & plot ###################################################
# DT_TrainResult1, DT_TestResult1, DT_TrainResult2, DT_TestResult2 = experiment_size(100, DTLearner)
# plotAccuray_TrainSize(DT_TrainResult1, DT_TestResult1,DT_TrainResult2, DT_TestResult2, "DecisionTree(TrainingSize)", "Training size(1%~99%)")
# DT_TrainResult_p1, DT_TestResult_p1, DT_TrainResult_p2, DT_TestResult_p2 = experiment_parameter(100, DTLearner_parameter)
# plotAccuray_parameter(DT_TrainResult_p1, DT_TestResult_p1,DT_TrainResult_p2, DT_TestResult_p2, "DecisionTree(parameter)", "Parameter:max_depth", 100)

# ############################################ Support Vector Machine ###################################################
# XXX
# Pre-process the data to standardize or normalize it, otherwise the grid search will take much longer
# Create a SVC classifier and train it.
# XXX
def SVMLearner(xdata, ydata, trainsize):
    xTrain, xTest, yTrain, yTest = train_test_split(xdata, ydata, train_size=trainsize, random_state=random_state)
    s = StandardScaler().fit(xTrain)
    xTrainSVC = s.transform(xTrain)
    xTestSVC = s.transform(xTest)
    svc = SVC().fit(xTrainSVC, yTrain)
    trainScore = accuracy_score(yTrain, svc.predict(xTrainSVC).round())
    testScore = accuracy_score(yTest, svc.predict(xTestSVC).round())
    print('SVM: trainScore', trainScore)
    print('SVM: testScoret', testScore)
    return trainScore, testScore

def SVMLearner_parameter(xTrain, xTest, yTrain, yTest, p):
    if(p > 4):
        return
    kernelList= ['rbf','linear', 'poly', 'sigmoid']
    s = StandardScaler().fit(xTrain)
    xTrainSVC = s.transform(xTrain)
    xTestSVC = s.transform(xTest)
    svc = SVC(kernel= kernelList[p-1]).fit(xTrainSVC, yTrain)
    trainScore = accuracy_score(yTrain, svc.predict(xTrainSVC).round())
    testScore = accuracy_score(yTest, svc.predict(xTestSVC).round())
    print('SVM: trainScore', trainScore)
    print('SVM: testScoret', testScore)
    return trainScore, testScore
# ###################################### Support Vector Machine Experiment with 2 dataset & plot ############################################
# SVM_TrainResult1,SVM_TestResult1,SVM_TrainResult2,SVM_TestResult2 = experiment_size(100, SVMLearner)
# plotAccuray_TrainSize(SVM_TrainResult1, SVM_TestResult1,SVM_TrainResult2, SVM_TestResult2, "SVM", "Training size(1%~99%)")
# SVM_TrainResult_p1,SVM_TestResult_p1,SVM_TrainResult_p2,SVM_TestResult_p2 = experiment_parameter(5, SVMLearner_parameter)
# plotAccuray_parameter(SVM_TrainResult_p1,SVM_TestResult_p1,SVM_TrainResult_p2,SVM_TestResult_p2, "SVM(parameter)", "Parameter: kernal ", 5)



# ############################################### K Nearest Neighbor(KNN) ###################################################
def KNNLearner(xdata, ydata, trainsize, k=3):
    xTrain, xTest, yTrain, yTest = train_test_split(xdata, ydata, train_size=trainsize, random_state=random_state)
    knn = KNeighborsClassifier(n_neighbors= k).fit(xTrain, yTrain)
    trainScore = accuracy_score(yTrain, knn.predict(xTrain).round())
    testScore = accuracy_score(yTest, knn.predict(xTest).round())
    print('KNN: trainScore', trainScore)
    print('KNN: testScoret', testScore)
    return trainScore, testScore

def KNNLearner_parameter(xTrain, xTest, yTrain, yTest, p):
    knn = KNeighborsClassifier(n_neighbors= p).fit(xTrain, yTrain)
    trainScore = accuracy_score(yTrain, knn.predict(xTrain).round())
    testScore = accuracy_score(yTest, knn.predict(xTest).round())
    print('KNN: trainScore', trainScore)
    print('KNN: testScoret', testScore)
    return trainScore, testScore
# ###################################### KNN Experiment with 2 dataset & plot ############################################
# knn_TrainResult1, knn_TestResult1, knn_TrainResult2, knn_TestResult2 = experiment_size(100, KNNLearner)
# plotAccuray_TrainSize(knn_TrainResult1, knn_TestResult1,knn_TrainResult2, knn_TestResult2, "KNN",  "Training size(1%~99%)")
# knn_TrainResult_p1, knn_TestResult_p1, knn_TrainResult_p2, knn_TestResult_p2 = experiment_parameter(15, KNNLearner_parameter)
# plotAccuray_parameter(knn_TrainResult_p1, knn_TestResult_p1, knn_TrainResult_p2, knn_TestResult_p2, "KNN(parameter)", "Parameter: k", 15)

# ############################################### Boosting: AdaBoost ###################################################
def BoostLearner(xdata, ydata, trainsize):
    xTrain, xTest, yTrain, yTest = train_test_split(xdata, ydata, train_size=trainsize, random_state=random_state)
    adb = AdaBoostClassifier(base_estimator= None).fit(xTrain, yTrain) #default is decision tree, n_estimators  = 50
    trainScore = accuracy_score(yTrain, adb.predict(xTrain).round())
    testScore = accuracy_score(yTest, adb.predict(xTest).round())
    print('Boosting: trainScore', trainScore)
    print('Boosting: testScoret', testScore)
    return trainScore, testScore
def BoostLearner_paramter(xTrain, xTest, yTrain, yTest, p):
    adb = AdaBoostClassifier(n_estimators = p).fit(xTrain, yTrain) #default is decision tree
    trainScore = accuracy_score(yTrain, adb.predict(xTrain).round())
    testScore = accuracy_score(yTest, adb.predict(xTest).round())
    print('Boosting: trainScore', trainScore)
    print('Boosting: testScoret', testScore)
    return trainScore, testScore
# ###################################### Boosting Experiment with 2 dataset & plot ############################################
# boost_TrainResult1, boost_TestResult1, boost_TrainResult2, boost_TestResult2 = experiment_size(100, BoostLearner)
# plotAccuray_TrainSize(boost_TrainResult1, boost_TestResult1, boost_TrainResult2, boost_TestResult2, "Boosting", "Training size(1%~99%)")
# boost_TrainResult_p1, boost_TestResult_p1, boost_TrainResult_p2, boost_TestResult_p2 = experiment_parameter(100, BoostLearner_paramter)
# plotAccuray_parameter(boost_TrainResult_p1, boost_TestResult_p1, boost_TrainResult_p2, boost_TestResult_p2, "Boosting(parameter)", "Parameter: k", 100)


# ############################################### Neural Network #######################################################
def NeuralNetworkLearner(xdata, ydata, trainsize):
    xTrain, xTest, yTrain, yTest = train_test_split(xdata, ydata, train_size=trainsize, random_state=random_state)
    nn = MLPClassifier(activation = 'logistic', solver = 'sgd', hidden_layer_sizes=(10,15), learning_rate_init= 0.001).fit(xTrain, yTrain) #training time cooresponding to hidden layer size
    trainScore = accuracy_score(yTrain, nn.predict(xTrain).round())
    testScore = accuracy_score(yTest, nn.predict(xTest).round())
    print('Neural Network: trainScore', trainScore)
    print('Neural Network: testScoret', testScore)
    return trainScore, testScore

def NeuralNetworkLearner_parameter(xTrain, xTest, yTrain, yTest, p):
    hiddenlayer = tuple(100 for i in range(0,p))
    nn = MLPClassifier(activation = 'logistic', solver = 'sgd', hidden_layer_sizes=hiddenlayer, learning_rate_init= 0.001).fit(xTrain, yTrain) #training time cooresponding to hidden layer size
    trainScore = accuracy_score(yTrain, nn.predict(xTrain).round())
    testScore = accuracy_score(yTest, nn.predict(xTest).round())
    print('Neural Network: trainScore', trainScore)
    print('Neural Network: testScoret', testScore)
    return trainScore, testScore
# ###################################### Neural Network Experiment with 2 dataset & plot ############################################
# nn_TrainResult1, nn_TestResult1, nn_TrainResult2, nn_TestResult2 = experiment_size(100, NeuralNetworkLearner)
# plotAccuray_TrainSize(nn_TrainResult1, nn_TestResult1,nn_TrainResult2, nn_TestResult2,  "Neural Network", "Training size(1%~99%)")
# nn_TrainResult1, nn_TestResult1, nn_TrainResult2, nn_TestResult2 = experiment_parameter(10, NeuralNetworkLearner_parameter)
# plotAccuray_parameter(nn_TrainResult1, nn_TestResult1,nn_TrainResult2, nn_TestResult2,  "Neural Network(parameter)","Parameter: hidden layer", 10)



# ###################################### Confusion Matrix: after determining the best size ###########################################
# DT_TrainResult1= [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
# DT_TestResult1= [0.858032276562628, 0.856835484938762, 0.8498453306579717, 0.852665371293402, 0.8492402253713505, 0.8468639461651282, 0.8565448678817477, 0.8516396332863188, 0.8564299082078246, 0.8573488330179327, 0.8502824858757062, 0.8536540411022026, 0.8602721849366145, 0.8610901546586194, 0.8629901726934452, 0.8573083606873914, 0.8657416454954074, 0.8576797547225794, 0.8606327593111734, 0.8622262773722628, 0.8622318037162509, 0.8539197338323976, 0.8609794628751974, 0.8682104364528865, 0.8639705882352942, 0.8641095890410959, 0.8645706032663037, 0.8592025230907863, 0.8595088520845231, 0.859460085737458, 0.8598965679360602, 0.8516398330351819, 0.8568143306705398, 0.8529122634553944, 0.8568933250155958, 0.8617587430309174, 0.8550463439752832, 0.8671026814911706, 0.8624036160595586, 0.8615842119491754, 0.86446735395189, 0.864513422818792, 0.8580167875942524, 0.8545981173062998, 0.8560896490710704, 0.8579366271211893, 0.8586074980872227, 0.861353711790393, 0.8549848942598187, 0.8493106244931062, 0.8576630254882489, 0.8592667680351411, 0.8545548654244306, 0.8552538787023978, 0.8576320057668049, 0.8623295245116107, 0.8615616748396832, 0.8596254103108708, 0.8619462025316456, 0.8548256285482563, 0.8581825743397796, 0.8606487409304311, 0.8612754766600921, 0.8578508673124577, 0.8593605189990732, 0.8616742189363225, 0.8584418776112067, 0.8580841358337558, 0.8579649489929375, 0.8642876453095432, 0.863255033557047, 0.8635968722849696, 0.866966966966967, 0.8593262632563943, 0.8602011028219267, 0.856081081081081, 0.857898448519041, 0.863251013638039, 0.862934362934363, 0.8686131386861314, 0.8711054204011951, 0.8545045045045045, 0.849308536003815, 0.8504815002534212, 0.8578378378378378, 0.85002895193978, 0.8546475358702433, 0.8486486486486486, 0.8511422254974208, 0.8532035685320357, 0.854954954954955, 0.8662613981762918, 0.8541666666666666, 0.8621621621621621, 0.8589951377633711, 0.8461538461538461, 0.8432432432432433, 0.8785425101214575, 0.8306451612903226]
# DT_TrainResult2= [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9997333333333334, 0.9997435897435898, 0.9997530864197531, 0.9996428571428572, 0.9996551724137931, 0.9996666666666667, 0.9996774193548387, 0.9996875, 0.9996969696969698, 0.9997058823529412, 0.9997142857142857, 0.9997222222222222, 0.9997297297297297, 0.9997368421052631, 0.9997435897435898, 0.9996666666666667, 0.9996747967479674, 0.9996825396825397, 0.9996124031007751, 0.9996212121212121, 0.9996296296296296, 0.9996376811594203, 0.999645390070922, 0.9996527777777777, 0.9996598639455783, 0.9996, 0.9996078431372549, 0.9996153846153846, 0.9996226415094339, 0.9996296296296296, 0.9996363636363637, 0.9996428571428572, 0.9996491228070176, 0.9996551724137931, 0.9996610169491525, 0.9996666666666667, 0.9996721311475409, 0.9996774193548387, 0.9996825396825397, 0.9996354166666667, 0.9996410256410256, 0.9995959595959596, 0.9996019900497513, 0.9995588235294117, 0.9995652173913043, 0.9995714285714286, 0.9995774647887324, 0.9995833333333334, 0.9995890410958904, 0.9995945945945945, 0.9996, 0.9996052631578948, 0.9995670995670995, 0.9995726495726496, 0.9995358649789029, 0.9995416666666667, 0.9995473251028807, 0.9995528455284552, 0.9995180722891567, 0.9995238095238095, 0.9994901960784314, 0.9994573643410852, 0.9994636015325671, 0.999469696969697, 0.999438202247191, 0.9994074074074074, 0.9994139194139194, 0.9993840579710145, 0.9993548387096775, 0.9993617021276596, 0.9993684210526316, 0.9993402777777778, 0.9993470790378007, 0.9993197278911564, 0.9993265993265993]
# DT_TestResult2= [0.7226599326599327, 0.7418027210884354, 0.7447422680412371, 0.7319791666666666, 0.7325964912280701, 0.7215602836879432, 0.7291756272401434, 0.7239855072463768, 0.7211721611721612, 0.7330740740740741, 0.7284269662921349, 0.7235227272727273, 0.7184674329501916, 0.7242248062015504, 0.7211372549019608, 0.7269047619047619, 0.7222088353413655, 0.7226422764227642, 0.7290946502057614, 0.7171666666666666, 0.7234599156118143, 0.722905982905983, 0.7258441558441558, 0.7216666666666667, 0.7258666666666667, 0.7251351351351352, 0.7223744292237443, 0.7183796296296296, 0.7249295774647887, 0.7254761904761905, 0.7225603864734299, 0.733578431372549, 0.72, 0.729040404040404, 0.7254358974358974, 0.7222395833333334, 0.7281481481481481, 0.7281182795698925, 0.7218579234972677, 0.7284444444444444, 0.7257062146892655, 0.7258045977011495, 0.724093567251462, 0.7244642857142857, 0.7248484848484849, 0.7303703703703703, 0.7290566037735849, 0.724423076923077, 0.7241830065359477, 0.7277333333333333, 0.734625850340136, 0.7275694444444445, 0.7325531914893617, 0.7232608695652174, 0.7262962962962963, 0.7295454545454545, 0.7230232558139534, 0.7275396825396825, 0.7266666666666667, 0.73, 0.7255555555555555, 0.7264035087719298, 0.73, 0.729537037037037, 0.7249523809523809, 0.7226470588235294, 0.7256565656565657, 0.7289583333333334, 0.7290322580645161, 0.7224444444444444, 0.7281609195402299, 0.7214285714285714, 0.7307407407407407, 0.728974358974359, 0.7194666666666667, 0.7290277777777778, 0.7221739130434782, 0.7234848484848485, 0.7212698412698413, 0.7291666666666666, 0.7177192982456141, 0.7192592592592593, 0.7170588235294117, 0.7210416666666667, 0.7213333333333334, 0.7238095238095238, 0.717948717948718, 0.7227777777777777, 0.7272727272727273, 0.7246666666666667, 0.7262962962962963, 0.7191666666666666, 0.7404761904761905, 0.7283333333333334, 0.724, 0.7075, 0.7233333333333334, 0.6983333333333334, 0.71]

# SVM_TrainResult1 = [0.926829268292683, 0.9146341463414634, 0.9051490514905149, 0.896551724137931, 0.8912337662337663, 0.8890392422192152, 0.8910776361529548, 0.8884381338742393, 0.8917944093778178, 0.8961881589618816, 0.8974926253687315, 0.8992562542258282, 0.900749063670412, 0.9026651216685979, 0.9010275824770146, 0.9006085192697769, 0.9002862595419847, 0.8999549346552501, 0.9009393680614859, 0.9018653690186537, 0.9061413673232909, 0.903023598820059, 0.9005291005291005, 0.9019939168638054, 0.9036340038935756, 0.9039001560062403, 0.9041754280564734, 0.9061413673232909, 0.9054545454545454, 0.9059205190592052, 0.9065934065934066, 0.9067173637515843, 0.908062930186824, 0.9072041984732825, 0.9079953650057937, 0.9076160432627309, 0.9061814993423937, 0.9062966915688367, 0.9059900166389351, 0.9053122465531225, 0.905835806132542, 0.906720741599073, 0.9060554612337295, 0.9063594470046082, 0.9068132660418169, 0.9060130488450009, 0.9057808455565143, 0.9046975329503211, 0.9039894057275285, 0.9051094890510949, 0.9047391857506362, 0.9045390734674777, 0.9044995408631772, 0.9052267948332833, 0.9065034655655508, 0.9054171494785631, 0.9053784860557769, 0.9049084044189624, 0.9044542205114106, 0.9034874290348743, 0.9040021273766786, 0.9042386185243328, 0.9039526200592249, 0.9040679254847295, 0.9044172697778887, 0.9042644709352341, 0.9040067788403341, 0.9032681297709924, 0.9037263430116375, 0.9037191518943344, 0.9031299977153301, 0.9036836769178777, 0.9032222222222223, 0.9035510740903112, 0.9038607115821348, 0.9030949839914622, 0.9034126816937013, 0.9032962462306332, 0.9029774127310062, 0.9030819140308192, 0.9034745168719335, 0.9029673590504451, 0.9030587315547738, 0.903157284928068, 0.903912213740458, 0.9029519947184759, 0.9032348280041018, 0.9028571428571428, 0.9023967921261278, 0.9024961701360729, 0.9026737967914439, 0.9027594110905405, 0.9019710448281877, 0.9019844693701466, 0.9025014940664219, 0.902500844880027, 0.902257525083612, 0.9016800463461061, 0.9025888907094871]
# SVM_TestResult1 = [0.853690505447694, 0.8677590201919894, 0.873087534487083, 0.8726873363183238, 0.8753628137271641, 0.8767146924337849, 0.8805267288741606, 0.8825811001410437, 0.8837893235896979, 0.8834820221681535, 0.8850920357207946, 0.8840659847018708, 0.8850671140939598, 0.8843832516031687, 0.8853162866138727, 0.8859818497779494, 0.8865546218487395, 0.8878449213727624, 0.8880656788145775, 0.88838199513382, 0.8872805666769326, 0.8879184861717613, 0.8888888888888888, 0.888272329527265, 0.8885164359861591, 0.8882191780821918, 0.8887901344295078, 0.8884883982879027, 0.8881781838949172, 0.8881937203105086, 0.8879877762106253, 0.8877757901013715, 0.8864681675139191, 0.8870729909068568, 0.8865876481597006, 0.887227572225038, 0.88697219361483, 0.8867233485938522, 0.8868651954267482, 0.8871316572046499, 0.8861855670103093, 0.8860458612975392, 0.8867548726703657, 0.887762490948588, 0.8864641698613979, 0.885268058267007, 0.8861514919663351, 0.8873986275733, 0.8890125616155191, 0.8880778588807786, 0.887785501489573, 0.8878188883257307, 0.8871635610766045, 0.8862834978843441, 0.8886285817264372, 0.8873940287504607, 0.8879668049792531, 0.8883954431357405, 0.888251582278481, 0.8884833738848338, 0.8904138074443751, 0.8900981647460521, 0.8897655051501205, 0.8884883982879027, 0.8897126969416126, 0.890770331504889, 0.8906365200294912, 0.8907754688291941, 0.8901386345801726, 0.889970262233036, 0.889261744966443, 0.8885027512308138, 0.8882882882882883, 0.8873986275733, 0.8871229322088875, 0.8891891891891892, 0.8889280677009873, 0.8875783265757464, 0.888030888030888, 0.8872668288726683, 0.8843363209560393, 0.8846846846846846, 0.882689556509299, 0.8849467815509376, 0.8821621621621621, 0.8836132020845396, 0.8827199001871491, 0.8824324324324324, 0.8835666912306559, 0.8905109489051095, 0.8918918918918919, 0.8915906788247214, 0.8946759259259259, 0.8986486486486487, 0.893030794165316, 0.8785425101214575, 0.8729729729729729, 0.8987854251012146, 0.8629032258064516]
# SVM_TrainResult2 = [0.8633333333333333, 0.875, 0.8655555555555555, 0.8583333333333333, 0.84, 0.8433333333333334, 0.839047619047619, 0.8383333333333334, 0.8381481481481482, 0.8363333333333334, 0.8342424242424242, 0.8344444444444444, 0.833076923076923, 0.8311904761904761, 0.828, 0.8270833333333333, 0.827843137254902, 0.8314814814814815, 0.83, 0.8296666666666667, 0.8288888888888889, 0.8301515151515152, 0.8292753623188406, 0.8288888888888889, 0.8278666666666666, 0.8285897435897436, 0.8306172839506173, 0.8289285714285715, 0.8277011494252874, 0.8281111111111111, 0.8279569892473119, 0.8279166666666666, 0.8278787878787879, 0.8265686274509804, 0.827047619047619, 0.8273148148148148, 0.8281981981981982, 0.8280701754385965, 0.8276068376068376, 0.8276666666666667, 0.826910569105691, 0.8273015873015873, 0.8274418604651163, 0.8276515151515151, 0.8277037037037037, 0.828695652173913, 0.8295744680851064, 0.8284027777777778, 0.8282993197278912, 0.8284, 0.828562091503268, 0.8280128205128205, 0.8282389937106919, 0.8274691358024692, 0.8271515151515152, 0.8281547619047619, 0.8275438596491228, 0.8278735632183908, 0.8278531073446328, 0.8279444444444445, 0.8282513661202185, 0.8279569892473119, 0.8276190476190476, 0.82703125, 0.8274358974358974, 0.8271717171717172, 0.8265671641791045, 0.8268627450980393, 0.8269082125603865, 0.8275714285714286, 0.8272769953051643, 0.8269444444444445, 0.8268493150684931, 0.8267567567567567, 0.8267111111111111, 0.8267543859649122, 0.827056277056277, 0.8271794871794872, 0.8269198312236287, 0.8265, 0.8262139917695474, 0.8261382113821139, 0.8265060240963855, 0.8259126984126984, 0.826078431372549, 0.8261240310077519, 0.8259386973180076, 0.82625, 0.8264794007490637, 0.8261481481481482, 0.8254578754578754, 0.8256159420289855, 0.8259856630824373, 0.8259219858156028, 0.8260701754385965, 0.8257638888888889, 0.8254639175257732, 0.8252721088435374, 0.824949494949495]
# SVM_TestResult2 = [0.7996296296296296, 0.808843537414966, 0.8102061855670103, 0.8129513888888888, 0.8124561403508772, 0.813049645390071, 0.8136917562724014, 0.8143478260869565, 0.814981684981685, 0.8147407407407408, 0.8166666666666667, 0.8176136363636364, 0.8174329501915709, 0.8184496124031008, 0.8184313725490197, 0.819484126984127, 0.8197188755020081, 0.8189430894308943, 0.8193004115226338, 0.8198333333333333, 0.819535864978903, 0.8197435897435897, 0.8195238095238095, 0.819078947368421, 0.8186666666666667, 0.8192342342342342, 0.8183561643835616, 0.8189814814814815, 0.8194366197183098, 0.8193333333333334, 0.8191787439613527, 0.8191666666666667, 0.8194029850746268, 0.8198989898989899, 0.8198461538461539, 0.8201041666666666, 0.8195767195767196, 0.8191397849462365, 0.8196174863387978, 0.8202777777777778, 0.8207344632768362, 0.8198275862068966, 0.8205263157894737, 0.8196428571428571, 0.8197575757575758, 0.8183333333333334, 0.8175471698113208, 0.8177564102564102, 0.8177777777777778, 0.8177333333333333, 0.817891156462585, 0.8184722222222223, 0.8176595744680851, 0.8180434782608695, 0.8185185185185185, 0.8179545454545455, 0.8181395348837209, 0.8184126984126984, 0.8182926829268292, 0.8181666666666667, 0.8178632478632478, 0.8177192982456141, 0.8182882882882883, 0.8191666666666667, 0.8179047619047619, 0.8181372549019608, 0.8191919191919191, 0.818125, 0.8188172043010753, 0.8176666666666667, 0.8179310344827586, 0.8184523809523809, 0.8166666666666667, 0.8183333333333334, 0.8186666666666667, 0.8181944444444444, 0.817536231884058, 0.8165151515151515, 0.8174603174603174, 0.8185, 0.8192982456140351, 0.8198148148148148, 0.817843137254902, 0.8208333333333333, 0.8195555555555556, 0.8183333333333334, 0.8205128205128205, 0.8161111111111111, 0.8142424242424242, 0.816, 0.8181481481481482, 0.8175, 0.8180952380952381, 0.8133333333333334, 0.8113333333333334, 0.8116666666666666, 0.8066666666666666, 0.8216666666666667, 0.8133333333333334]

# knn_TrainResult1= [0.9186991869918699, 0.9065040650406504, 0.8997289972899729, 0.8904665314401623, 0.8847402597402597, 0.8917456021650879, 0.8910776361529548, 0.8935091277890467, 0.8935978358881875, 0.8978102189781022, 0.8997050147492626, 0.9019607843137255, 0.900749063670412, 0.90324449594438, 0.897782585181179, 0.8975659229208925, 0.898854961832061, 0.8986029743127535, 0.8979504696840307, 0.9002433090024331, 0.9042101197373503, 0.9015486725663717, 0.9022927689594357, 0.9030077728962488, 0.9010382868267359, 0.9014040561622465, 0.9032742565334936, 0.9038238702201622, 0.9046153846153846, 0.9048391457150581, 0.9050235478806907, 0.9039290240811153, 0.9041297935103245, 0.9033874045801527, 0.9040556199304751, 0.9044614691302388, 0.9039894782989917, 0.9052294557097118, 0.9055740432612313, 0.9053122465531225, 0.9062314540059347, 0.9063344920818849, 0.9066213921901528, 0.9082027649769585, 0.9086157173756308, 0.9093634279668489, 0.9090595340811044, 0.9085839810746874, 0.9087899354411522, 0.9096512570965126, 0.9090330788804071, 0.9096864763687412, 0.9098561371288644, 0.9100330429558426, 0.9101902374281079, 0.9100521436848203, 0.9105008537279453, 0.9100825059432247, 0.9106406378883696, 0.9110570424439037, 0.9095864911580906, 0.9085557299843015, 0.9074288657139179, 0.9086300849068559, 0.9089094085350636, 0.9090573921592725, 0.9087277569301537, 0.9088740458015268, 0.9087810038791584, 0.9089329162321863, 0.9095270733379027, 0.9094288611017236, 0.9097777777777778, 0.9101271372205173, 0.9106737320211961, 0.9103521878335112, 0.9111017484727196, 0.911302901112613, 0.9120123203285421, 0.9121046228710462, 0.9115850605787523, 0.9113748763600396, 0.9112674679957001, 0.9118470599594477, 0.9113549618320611, 0.911062906724512, 0.9114384264006712, 0.9113364055299539, 0.9105987423676296, 0.9106064702171758, 0.9110516934046345, 0.911134620470775, 0.9105180533751962, 0.9106125970664366, 0.9104413899086485, 0.9112875971612031, 0.9113712374581939, 0.9109492675660018, 0.9113550712764215]
# knn_TestResult1=[0.8065863848611452, 0.8286991062562066, 0.8253490510826854, 0.8317141167525556, 0.829434864264982, 0.8283150720386506, 0.8310804918461673, 0.8293370944992948, 0.830050797611621, 0.8322069027665134, 0.8314197193366138, 0.8353147175375542, 0.8356636838180462, 0.8371369294605809, 0.839709951340521, 0.8399304885112956, 0.8400429939417627, 0.8417565028187123, 0.8441129355226271, 0.845904298459043, 0.8469356328919002, 0.8459139114160948, 0.8479199578725645, 0.8488955287589371, 0.8467776816608996, 0.8463561643835616, 0.849127874680591, 0.8488398287902681, 0.8492290119931468, 0.8479898041941838, 0.847672778561354, 0.8492546213476446, 0.8491890583393852, 0.8509461784222168, 0.8507797878976918, 0.8506082108464268, 0.8500257466529351, 0.8512753433616743, 0.8522999202339804, 0.8532035685320357, 0.8529209621993127, 0.8530480984340044, 0.8528951486697965, 0.8515568428674873, 0.8521085225597169, 0.8504279921910197, 0.8501912777352716, 0.8504366812227074, 0.8508506916838925, 0.8496350364963504, 0.8508771929824561, 0.8511572900827843, 0.8502415458937198, 0.8508462623413258, 0.8500630744278248, 0.8492443789163288, 0.8500565824217277, 0.8493917744738366, 0.848496835443038, 0.8491484184914841, 0.851112497400707, 0.8510456679470764, 0.8516326977865439, 0.8528947961252534, 0.852873030583874, 0.8535654662532792, 0.8537724256574096, 0.8527622909275215, 0.8522103060423751, 0.8513111651797783, 0.8515100671140939, 0.8505647263249348, 0.8531531531531531, 0.8521522145976295, 0.850145961725592, 0.8513513513513513, 0.8494358251057827, 0.8477699963140435, 0.8478764478764479, 0.8495539334955393, 0.8518992744344857, 0.8522522522522522, 0.8526466380543634, 0.8504815002534212, 0.8556756756756757, 0.8598726114649682, 0.860885839051778, 0.8601351351351352, 0.8592483419307295, 0.862124898621249, 0.8639639639639639, 0.8652482269503546, 0.8668981481481481, 0.8621621621621621, 0.8638573743922204, 0.8562753036437247, 0.8540540540540541, 0.8704453441295547, 0.8467741935483871]
# knn_TrainResult2= [0.85, 0.8666666666666667, 0.8455555555555555, 0.8458333333333333, 0.8373333333333334, 0.8338888888888889, 0.8342857142857143, 0.83875, 0.8348148148148148, 0.8393333333333334, 0.8436363636363636, 0.8422222222222222, 0.8464102564102564, 0.8459523809523809, 0.8462222222222222, 0.84625, 0.8454901960784313, 0.8444444444444444, 0.8440350877192982, 0.8406666666666667, 0.8425396825396826, 0.8436363636363636, 0.8439130434782609, 0.8447222222222223, 0.8466666666666667, 0.8466666666666667, 0.8471604938271605, 0.8458333333333333, 0.8459770114942529, 0.847, 0.8472043010752688, 0.8475, 0.8480808080808081, 0.8479411764705882, 0.8478095238095238, 0.8476851851851852, 0.8464864864864865, 0.8466666666666667, 0.8464102564102564, 0.8460833333333333, 0.8450406504065041, 0.8442857142857143, 0.8450387596899225, 0.8452272727272727, 0.8462222222222222, 0.8457246376811595, 0.845886524822695, 0.8459027777777778, 0.8454421768707483, 0.8453333333333334, 0.8447712418300654, 0.844551282051282, 0.8442767295597484, 0.8435185185185186, 0.8424242424242424, 0.8423809523809523, 0.8428070175438597, 0.8441379310344828, 0.8442372881355932, 0.8451111111111111, 0.8453551912568306, 0.845268817204301, 0.8450793650793651, 0.8459375, 0.8466666666666667, 0.8462121212121212, 0.8459701492537314, 0.8462745098039216, 0.8468115942028985, 0.8460952380952381, 0.8459154929577465, 0.8464351851851852, 0.846986301369863, 0.8469819819819819, 0.8466222222222223, 0.8467543859649123, 0.8473160173160174, 0.8465384615384616, 0.8469198312236287, 0.8469166666666667, 0.847037037037037, 0.8463414634146341, 0.8466265060240964, 0.8463492063492063, 0.8462745098039216, 0.8462015503875969, 0.8465900383141762, 0.8465530303030303, 0.8463670411985019, 0.8461851851851852, 0.8461172161172161, 0.8464855072463768, 0.8471326164874552, 0.8469148936170213, 0.8472280701754386, 0.8469444444444445, 0.8462886597938144, 0.8464965986394558, 0.8458249158249158]
# knn_TestResult2= [0.7315151515151516, 0.7281292517006803, 0.7235395189003436, 0.7275, 0.7163859649122807, 0.7219148936170213, 0.715089605734767, 0.72, 0.7256043956043956, 0.7258888888888889, 0.7263295880149813, 0.7279545454545454, 0.7268582375478927, 0.7273643410852714, 0.727764705882353, 0.7246428571428571, 0.7246987951807229, 0.7239837398373984, 0.7267901234567902, 0.7285, 0.7290717299578059, 0.7302564102564103, 0.7325108225108226, 0.7330263157894736, 0.7330222222222222, 0.7338738738738739, 0.7344292237442922, 0.7352777777777778, 0.734225352112676, 0.7355714285714285, 0.7343478260869565, 0.7341666666666666, 0.7339303482587065, 0.7338888888888889, 0.7341538461538462, 0.735, 0.7352380952380952, 0.7348387096774194, 0.7344808743169399, 0.7336666666666667, 0.7335028248587571, 0.7330459770114942, 0.7339181286549707, 0.7339285714285714, 0.7332727272727273, 0.7324691358024691, 0.7313207547169811, 0.7303846153846154, 0.7305882352941176, 0.7318, 0.7321768707482993, 0.7324305555555556, 0.7324822695035461, 0.7342028985507246, 0.7355555555555555, 0.7353787878787879, 0.7353488372093023, 0.7357936507936508, 0.735040650406504, 0.73475, 0.7344444444444445, 0.7331578947368421, 0.733063063063063, 0.7310185185185185, 0.7317142857142858, 0.7316666666666667, 0.7329292929292929, 0.7322916666666667, 0.7308602150537634, 0.7293333333333333, 0.7274712643678161, 0.7296428571428571, 0.727037037037037, 0.7283333333333334, 0.7304, 0.7311111111111112, 0.731304347826087, 0.7319696969696969, 0.7320634920634921, 0.7356666666666667, 0.7356140350877193, 0.7362962962962963, 0.735686274509804, 0.7366666666666667, 0.7357777777777778, 0.7359523809523809, 0.7402564102564102, 0.7375, 0.7345454545454545, 0.734, 0.7337037037037037, 0.7320833333333333, 0.729047619047619, 0.7261111111111112, 0.726, 0.7241666666666666, 0.7244444444444444, 0.7166666666666667, 0.72]

# boost_TrainResult1= [1.0, 1.0, 0.978319783197832, 0.9614604462474645, 0.9431818181818182, 0.9377537212449256, 0.9258400926998841, 0.9239350912778904, 0.9170423805229937, 0.9213300892133008, 0.9166666666666666, 0.9134550371872887, 0.9101123595505618, 0.9107763615295481, 0.9064359113034073, 0.9082150101419878, 0.9093511450381679, 0.9044614691302388, 0.9047822374039283, 0.9030819140308192, 0.9042101197373503, 0.9056047197640118, 0.9044091710758377, 0.9006421088205475, 0.8987670343932511, 0.9001560062402496, 0.899969960949234, 0.9003476245654692, 0.8973426573426574, 0.8994322789943228, 0.8984824699110413, 0.8958174904942966, 0.8930678466076696, 0.8959923664122137, 0.895712630359212, 0.8990536277602523, 0.8971942130644455, 0.8947705442902881, 0.8951747088186356, 0.8943633414436334, 0.895944609297725, 0.8980301274623407, 0.8930390492359932, 0.8943778801843318, 0.8929343907714492, 0.8933168753306295, 0.8937014667817084, 0.8943899966204799, 0.8943883463002814, 0.8940794809407948, 0.8958333333333334, 0.8934643581344563, 0.894092439546985, 0.8965154701111445, 0.8972128004719068, 0.8948435689455388, 0.8927148548662492, 0.8955390854425954, 0.8944184767665658, 0.895241957285753, 0.8945618933652439, 0.8927263212977499, 0.8923651345435818, 0.893422886833101, 0.8914399800349389, 0.8930809880791446, 0.8928701125771699, 0.8942032442748091, 0.8933819207711297, 0.8938709303672807, 0.8955905871601554, 0.8933198152529007, 0.8947777777777778, 0.8960982025427444, 0.894776684330053, 0.8944503735325507, 0.8937223509585001, 0.8940418009774358, 0.8958932238193018, 0.8975060827250608, 0.8962651446880945, 0.8943620178041543, 0.8945568259552429, 0.8962054649029642, 0.8950381679389313, 0.8942752051306234, 0.8937261116808054, 0.8945622119815668, 0.8940125763237037, 0.8940254122735874, 0.8950089126559715, 0.8935907608216521, 0.8946450375021804, 0.8944779982743745, 0.8946469734483053, 0.894812436633998, 0.8936454849498328, 0.8936522386824464, 0.8934130755366213]
# boost_TestResult1= [0.861472925370689, 0.8709864283349884, 0.8644762143633475, 0.8758976091915182, 0.8791190029025098, 0.8830126822534725, 0.8853231010726432, 0.8840796897038082, 0.8829872560377863, 0.8805082454717491, 0.8810825587752871, 0.8847110865358031, 0.8862788963460104, 0.8862693323274237, 0.8873199122221163, 0.8858853060436378, 0.8860660543287082, 0.8847789536148749, 0.8882659191029235, 0.8847323600973236, 0.883687506416179, 0.885215221459763, 0.8855186940494998, 0.8881656173300608, 0.8897058823529411, 0.8883287671232877, 0.888012443061882, 0.8900653300292859, 0.8924043403769275, 0.8908585331942996, 0.891161259990597, 0.8909958258795468, 0.8909465020576132, 0.8898992381420496, 0.8903306300686213, 0.8891282311201216, 0.8898043254376931, 0.8903858731196861, 0.8895240627492688, 0.891051635577183, 0.8916838487972508, 0.8907997762863534, 0.8887466211409873, 0.8919623461259957, 0.8898554998525509, 0.8872203033488512, 0.8895179801071156, 0.8881784154709919, 0.8878995070758468, 0.8869424168694242, 0.8881165177093677, 0.8869741510390269, 0.8864734299516909, 0.8887517630465445, 0.8893494323301496, 0.8873940287504607, 0.8887212372689551, 0.8887816180729871, 0.8866693037974683, 0.889294403892944, 0.8875025992929924, 0.8890311566367904, 0.893710278325663, 0.8920928136967785, 0.8913345690454124, 0.8924397805866922, 0.889161956254608, 0.892295995945261, 0.8898770599006016, 0.8880778588807786, 0.886744966442953, 0.8916883869099334, 0.8864864864864865, 0.8867747972551466, 0.8884203697697048, 0.8875, 0.8871650211565585, 0.8846295613711758, 0.883011583011583, 0.8815896188158961, 0.8809218950064021, 0.8797297297297297, 0.8798283261802575, 0.8783578307146478, 0.8810810810810811, 0.8824551244933411, 0.8796007485963818, 0.8783783783783784, 0.8806190125276345, 0.8864557988645579, 0.8873873873873874, 0.8865248226950354, 0.8900462962962963, 0.8972972972972973, 0.8897893030794165, 0.8785425101214575, 0.8756756756756757, 0.902834008097166, 0.8306451612903226]
# boost_TrainResult2 = [0.9, 0.88, 0.8588888888888889, 0.8608333333333333, 0.8426666666666667, 0.8405555555555555, 0.8357142857142857, 0.835, 0.8322222222222222, 0.8323333333333334, 0.8336363636363636, 0.8322222222222222, 0.8325641025641025, 0.8271428571428572, 0.8275555555555556, 0.8227083333333334, 0.8219607843137255, 0.8266666666666667, 0.8249122807017544, 0.823, 0.8234920634920635, 0.8259090909090909, 0.8240579710144927, 0.8230555555555555, 0.8232, 0.821025641025641, 0.8230864197530864, 0.8213095238095238, 0.8234482758620689, 0.824, 0.8260215053763441, 0.8247916666666667, 0.8204040404040404, 0.8211764705882353, 0.823047619047619, 0.8219444444444445, 0.8227027027027027, 0.8235087719298245, 0.8225641025641026, 0.8221666666666667, 0.8213821138211382, 0.8212698412698413, 0.8210852713178295, 0.8217424242424243, 0.8216296296296296, 0.8228985507246377, 0.8241134751773049, 0.8245833333333333, 0.8224489795918367, 0.821, 0.8215032679738562, 0.8223076923076923, 0.8230188679245283, 0.8214814814814815, 0.8218181818181818, 0.8184523809523809, 0.8211111111111111, 0.8183333333333334, 0.81954802259887, 0.8195, 0.8201639344262295, 0.8225806451612904, 0.8203703703703704, 0.8182291666666667, 0.8200512820512821, 0.8173232323232323, 0.817910447761194, 0.8191666666666667, 0.8194685990338164, 0.8195238095238095, 0.8195305164319249, 0.8188425925925926, 0.8190867579908676, 0.8188288288288288, 0.8181777777777778, 0.8184210526315789, 0.8181385281385282, 0.8184188034188035, 0.8187341772151899, 0.8182916666666666, 0.8174897119341564, 0.8182113821138212, 0.8183534136546184, 0.8167460317460318, 0.8170980392156862, 0.8178294573643411, 0.8173180076628352, 0.816969696969697, 0.817565543071161, 0.8178148148148148, 0.8183516483516483, 0.8176086956521739, 0.8174910394265233, 0.8182978723404255, 0.8174385964912281, 0.8185416666666666, 0.8185223367697595, 0.8182312925170068, 0.8173063973063973]
# boost_TestResult2= [0.7914478114478114, 0.8056802721088435, 0.8110996563573883, 0.8046527777777778, 0.8095789473684211, 0.8112411347517731, 0.8127240143369175, 0.8134057971014492, 0.8120512820512821, 0.8112222222222222, 0.8123220973782772, 0.8128787878787879, 0.8121072796934866, 0.8146124031007752, 0.8164313725490197, 0.8163492063492064, 0.8171887550200804, 0.8172764227642276, 0.817201646090535, 0.81525, 0.8171729957805908, 0.8151709401709402, 0.8151948051948052, 0.8135526315789474, 0.8150222222222222, 0.8144144144144144, 0.8128767123287671, 0.8153703703703704, 0.8158215962441314, 0.8173333333333334, 0.8171980676328502, 0.818578431372549, 0.814726368159204, 0.8161111111111111, 0.8172307692307692, 0.8171354166666667, 0.8175661375661376, 0.8175806451612904, 0.8175409836065574, 0.8181666666666667, 0.8190395480225988, 0.8188505747126437, 0.8178947368421052, 0.8191071428571428, 0.8184242424242424, 0.8172222222222222, 0.8163522012578617, 0.8162820512820513, 0.8168627450980392, 0.8136666666666666, 0.8150340136054421, 0.81625, 0.8157446808510638, 0.816304347826087, 0.8168148148148148, 0.8141666666666667, 0.8167441860465117, 0.8142063492063492, 0.8145528455284553, 0.8136666666666666, 0.8116239316239316, 0.8142105263157895, 0.8141441441441442, 0.8144444444444444, 0.8162857142857143, 0.8150980392156862, 0.8154545454545454, 0.8159375, 0.8156989247311828, 0.8134444444444444, 0.8159770114942528, 0.8152380952380952, 0.8146913580246914, 0.8170512820512821, 0.8157333333333333, 0.81625, 0.8157971014492753, 0.8159090909090909, 0.8176190476190476, 0.8188333333333333, 0.8180701754385965, 0.817962962962963, 0.8188235294117647, 0.8202083333333333, 0.82, 0.8185714285714286, 0.8215384615384616, 0.8177777777777778, 0.816060606060606, 0.818, 0.8185185185185185, 0.8191666666666667, 0.8176190476190476, 0.8138888888888889, 0.808, 0.81, 0.81, 0.8133333333333334, 0.81]

# nn_TrainResult1= [0.8699186991869918, 0.8617886178861789, 0.8536585365853658, 0.847870182555781, 0.8441558441558441, 0.8470906630581867, 0.8424101969872537, 0.8356997971602435, 0.8385933273219116, 0.8434712084347121, 0.8473451327433629, 0.847870182555781, 0.848314606741573, 0.847045191193511, 0.8458626284478096, 0.8427991886409736, 0.8444656488549618, 0.8427219468228931, 0.840734415029889, 0.8406326034063261, 0.8439551950560061, 0.842551622418879, 0.8419753086419753, 0.8428523149712741, 0.8442569759896171, 0.8458658346333854, 0.8452988885551217, 0.8450173812282734, 0.8436363636363636, 0.8448229251148959, 0.8456305599162742, 0.8456273764258555, 0.8466076696165191, 0.8454198473282443, 0.8456546929316339, 0.846777827850383, 0.8483121437965805, 0.8486659551760939, 0.8483777038269551, 0.8477291159772912, 0.8482690405539071, 0.8489764387794515, 0.8490850782871157, 0.8482949308755761, 0.8493150684931506, 0.8497619467466055, 0.8483175150992235, 0.8480905711388983, 0.8477073332229763, 0.8478507704785077, 0.84764631043257, 0.8468257682108875, 0.8468013468013468, 0.8472514268549114, 0.8464828196431204, 0.8458864426419467, 0.8453329538986909, 0.8454761571808139, 0.845889469342865, 0.8456339551230062, 0.845100385587023, 0.8445839874411303, 0.8439551950560061, 0.8438727664427829, 0.8435238332917394, 0.8430625537667445, 0.8425130129524271, 0.8423187022900763, 0.8421300105795227, 0.8419650098482215, 0.8422435458076308, 0.8427396643010026, 0.843, 0.8431608943445857, 0.8429761003568725, 0.8426894343649947, 0.843164103644407, 0.8436102734740564, 0.8438398357289528, 0.8443836171938361, 0.8443977170321418, 0.8441147378832838, 0.844522622886739, 0.8445495799942068, 0.8449427480916031, 0.8448552296519853, 0.8451570802647524, 0.8455299539170507, 0.845165405996537, 0.8447328106695503, 0.8444741532976827, 0.845014546416292, 0.8449328449328449, 0.8446937014667817, 0.8451293434645266, 0.8455559310577898, 0.8457357859531772, 0.8454026317967392, 0.8456496804850073]
# nn_TestResult1= [0.8450069632178259, 0.8449189010261503, 0.8449962377727614, 0.8451465743009209, 0.8453133003243981, 0.8451384695022, 0.8454696084416151, 0.8460860366713682, 0.8459139114160948, 0.8454537262323151, 0.844997266265719, 0.844899087641692, 0.8447986577181208, 0.8449641644662391, 0.8451483637057533, 0.8457231125699942, 0.8454172366621067, 0.8458114924339828, 0.8463155786944333, 0.8464111922141119, 0.8456010676521918, 0.8460178831357871, 0.8462348604528699, 0.8460142994344254, 0.8455882352941176, 0.845041095890411, 0.8452394178424619, 0.8453480513629196, 0.8459166190748144, 0.8454408527401228, 0.8450869769628585, 0.8450805008944544, 0.8445896877269427, 0.8451708036372573, 0.8450405489706799, 0.8443993917891536, 0.84346035015448, 0.8431654676258993, 0.8432597713374103, 0.8436063801027305, 0.8431615120274915, 0.8425615212527964, 0.8423673353250818, 0.8428674873280232, 0.8419345325862577, 0.8414176302748161, 0.842540168324407, 0.8426388022457891, 0.8429003021148036, 0.8426601784266018, 0.8427672955974843, 0.8435546545024497, 0.8435127674258109, 0.8429125528913963, 0.8437556316453415, 0.8444526354589016, 0.8451527725386646, 0.8449507626955011, 0.8443433544303798, 0.8446877534468775, 0.8454980245373258, 0.8463508322663252, 0.8474687705456936, 0.8477134489749943, 0.8484708063021316, 0.8495110899117577, 0.8508232981076431, 0.8514951849974658, 0.8522103060423751, 0.8529332251959989, 0.8526286353467561, 0.8517231392991601, 0.8513513513513513, 0.8512164691203993, 0.8520921180668181, 0.8533783783783784, 0.8522566995768688, 0.8510873571691854, 0.8505791505791506, 0.8487429034874291, 0.8489116517285531, 0.8504504504504504, 0.848831664282308, 0.8489609731373543, 0.847027027027027, 0.8477127967573828, 0.8459139114160948, 0.8432432432432433, 0.8459837877671333, 0.8499594484995945, 0.8531531531531531, 0.8480243161094225, 0.8495370370370371, 0.8540540540540541, 0.8476499189627229, 0.8380566801619433, 0.8297297297297297, 0.8380566801619433, 0.8064516129032258]
# nn_TrainResult2= [0.8133333333333334, 0.8083333333333333, 0.8022222222222222, 0.7958333333333333, 0.7846666666666666, 0.7872222222222223, 0.7828571428571428, 0.7854166666666667, 0.7888888888888889, 0.7876666666666666, 0.7863636363636364, 0.7844444444444445, 0.7838461538461539, 0.7842857142857143, 0.7822222222222223, 0.7810416666666666, 0.7819607843137255, 0.7831481481481481, 0.7828070175438596, 0.784, 0.7836507936507936, 0.7846969696969697, 0.783768115942029, 0.7838888888888889, 0.784, 0.785, 0.7858024691358024, 0.7838095238095238, 0.7836781609195402, 0.7834444444444445, 0.7826881720430108, 0.783125, 0.7829292929292929, 0.7820588235294118, 0.7825714285714286, 0.7825, 0.7826126126126126, 0.7820175438596492, 0.7818803418803418, 0.7810833333333334, 0.7807317073170732, 0.7807936507936508, 0.7810852713178295, 0.7821969696969697, 0.7824444444444445, 0.7835507246376812, 0.7843262411347518, 0.784375, 0.7842857142857143, 0.7843333333333333, 0.784640522875817, 0.7841666666666667, 0.7841509433962264, 0.783641975308642, 0.7830303030303031, 0.7836904761904762, 0.7830409356725146, 0.7828735632183909, 0.7831073446327683, 0.7827777777777778, 0.7833879781420765, 0.7832795698924732, 0.782962962962963, 0.78265625, 0.7831794871794872, 0.782979797979798, 0.7825373134328358, 0.7823039215686275, 0.782463768115942, 0.7827142857142857, 0.7826291079812207, 0.7825925925925926, 0.782648401826484, 0.7817567567567567, 0.7816888888888889, 0.7816666666666666, 0.7815584415584416, 0.7811965811965812, 0.7810126582278482, 0.7807916666666667, 0.7810699588477367, 0.7807317073170732, 0.7810441767068274, 0.7805555555555556, 0.7806666666666666, 0.7805426356589147, 0.78, 0.7802651515151515, 0.780374531835206, 0.780074074074074, 0.7798901098901099, 0.7798188405797102, 0.7801075268817205, 0.7801418439716312, 0.7800701754385965, 0.7796527777777778, 0.7795189003436426, 0.7792857142857142, 0.7792929292929293]
# nn_TestResult2 =[0.7784511784511785, 0.7781972789115646, 0.7780756013745704, 0.7780902777777777, 0.7784912280701755, 0.7782624113475177, 0.7784946236559139, 0.7782246376811595, 0.7778021978021978, 0.7778148148148148, 0.7778651685393259, 0.7780303030303031, 0.7780459770114942, 0.7779069767441861, 0.7781960784313725, 0.7783730158730159, 0.7781526104417671, 0.7778455284552845, 0.7778600823045267, 0.7775, 0.7775105485232068, 0.7771367521367522, 0.7773160173160173, 0.7771929824561403, 0.7770666666666667, 0.7766216216216216, 0.7762100456621005, 0.7768518518518519, 0.7768075117370892, 0.7768095238095238, 0.7770531400966184, 0.7767647058823529, 0.7767661691542288, 0.7771212121212121, 0.7767692307692308, 0.77671875, 0.7765608465608466, 0.7768279569892473, 0.7768306010928961, 0.7772777777777777, 0.7774576271186441, 0.7773563218390804, 0.7770760233918129, 0.7761309523809524, 0.7758181818181819, 0.774753086419753, 0.7738993710691824, 0.7736538461538461, 0.7735294117647059, 0.7732666666666667, 0.7727210884353741, 0.7729861111111112, 0.7727659574468085, 0.7731159420289855, 0.7736296296296297, 0.7725757575757576, 0.7731782945736434, 0.7731746031746032, 0.7726016260162601, 0.7728333333333334, 0.7716239316239316, 0.7714912280701754, 0.7717117117117117, 0.7719444444444444, 0.7706666666666667, 0.7706862745098039, 0.7712121212121212, 0.7713541666666667, 0.7706451612903226, 0.7696666666666667, 0.7694252873563219, 0.7690476190476191, 0.7683950617283951, 0.7703846153846153, 0.7701333333333333, 0.7697222222222222, 0.7695652173913043, 0.7703030303030303, 0.7704761904761904, 0.7708333333333334, 0.7691228070175439, 0.77, 0.7678431372549019, 0.7695833333333333, 0.7682222222222223, 0.768095238095238, 0.7707692307692308, 0.7680555555555556, 0.7660606060606061, 0.7673333333333333, 0.7677777777777778, 0.7670833333333333, 0.7614285714285715, 0.7577777777777778, 0.7546666666666667, 0.7583333333333333, 0.7555555555555555, 0.755, 0.73]
